{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWgujXmGqzIC"
   },
   "source": [
    "# Lab 8 - Aspect-Based Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr5PWYKGPi6R"
   },
   "source": [
    "In this lab we turn a pre-trained BERT model into a trainable Keras layer and apply it to the Aspect-Based Sentiment Analysis that we tackled in lab 4. BERT (Bidirectional Embedding Representations from Transformers) is a new model for pre-training language representations that obtains state-of-the-art results on many NLP tasks. We demonstrate how to integrate BERT as a custom Keras layer to simplify model prototyping using huggingface. In this lab, you will learn: \n",
    "\n",
    "1) How to use the huggingface package.\n",
    "\n",
    "2) How to integrate BERT in our previous model. \n",
    "\n",
    "3) How to use the TPU from Colab. (Note: Running BERT on the CPU would be very slow. Thus we recommend you to do this lab on Colab based on TPU provided by Google.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4spJ5PRhGJ1l"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oimYsLssuSs"
   },
   "source": [
    "Before start, we should install the huggingface transformer package. You can find the doc from its [website](https://huggingface.co/transformers/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AElpzsKFiZSo",
    "outputId": "8a2eb73e-4431-41c8-c6ed-312585570922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 4.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 46.3 MB/s \n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 33.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 4.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 40.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebbamBD0xu5z"
   },
   "source": [
    "## Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbAMXQwqyVT8"
   },
   "source": [
    "In this lab we will use DistilBERT instead of BERT: DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, and runs 60% faster, while preserving over 95% of BERT’s performance as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "It is easy to switch between DistilBERT and BERT using the huggingface transformer package. This huggingface package provides many pre-trained and pre-built models that are easy to use via a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-cUTxt02dQb"
   },
   "source": [
    "Before using DistilBERT or BERT, we need a tokenizer. Generally speaking, every BERT related model has its own tokenizer, trained for that model (see this week's lecture video on sub-word tokenization). \n",
    "We can get the DistilBERT tokenizer from **DistilBertTokenizer.from_pretrained** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "0b454e3aa10449bab0e12a0596956a76",
      "9360dea7d34a4990936148f6169c3824",
      "922efe9c76364b7b8f34cd096954bbf4",
      "0ee4a6d9b79447a6b9261b36136715d7",
      "ea6d50c398b84282a48de27658bcbb9d",
      "7bb3662f2166440f91e513d50d95fbe6",
      "b362747df7ce429590a80af9acac1a0f",
      "f188e328fb3c45868a0b581fdf477a48",
      "fc3387a8d78d479e973f45e958a79cef",
      "c747fa346cb7406784f7aacf5bb511f4",
      "2567d8842c18498695baab8eb5a3cfc4",
      "cf7d0ccad32c46a18eeb56f53bd7ee68",
      "3c04dde3e07049779e31da6db6c621da",
      "3fec1d86d6e741c6942ca6fe379681ff",
      "63a31d3ca8ea4d82acf7bbc99f9eaec8",
      "9e148d92e6e740bcb80e95890cb8ba4b",
      "fcce4986a120492b830dadcfe16d8b61",
      "66d5c015f9e34865aa14526f4f2b2381",
      "76af8cde39884aaa8b90716f60249141",
      "484a90ceaf5840cc95dc8fee9852a9f7",
      "e94f80f6c6864a40ba4f423277582fdd",
      "425468b3eed24365aad81b0e40119d16",
      "c3438d18a2bb4e29979a708a1b763f0c",
      "cdbd7d8b506c4209a6ffccc25e927522",
      "0a5376e1c80b440fa241b974e2cf656f",
      "f5f557c5831b4e839d30d7ee3ba129ed",
      "fe244c1ce9d24c1680a1f1ff382b2c32",
      "2f84d0392d2240ee8ab110b14f147135",
      "64fedb8aa29f4d598c2402b421fb95ad",
      "d3622e198da148a9b1bfcc14f5b27225",
      "831498ac7a1d4b59a1c1174df5d2a7b0",
      "68cfc6b4ef8f4e8383453cd37c0f4261",
      "a0bf941ef39d4e648cb2e8f50cd6c5e8"
     ]
    },
    "id": "hKj6Y_TydjeS",
    "outputId": "3a0d0c3a-fc58-4c95-eb17-24c0532c13c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b454e3aa10449bab0e12a0596956a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7d0ccad32c46a18eeb56f53bd7ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3438d18a2bb4e29979a708a1b763f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, RobertaTokenizer \n",
    "import tqdm\n",
    "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
    "\n",
    "# Defining DistilBERT tokonizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=128, pad_to_max_length=True)\n",
    "\n",
    "def tokenize(sentences, tokenizer, pad_length=128, pad_to_max_length=True ):\n",
    "    if type(sentences) == str:\n",
    "        inputs = tokenizer.encode_plus(sentences, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        return np.asarray(inputs['input_ids'], dtype='int32'), np.asarray(inputs['attention_mask'], dtype='int32'), np.asarray(inputs['token_type_ids'], dtype='int32')\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqMo6CPS3qJZ"
   },
   "source": [
    "Then we can use the tokenizer to tokenize the sentence. When working with word2vec and GloVe, we tokenized the sentence into words ourselves and then converted the tokens to GloVe word indices. But in BERT, we must use the BERT tokenizer: the tokens for BERT are different, and include whole words and sub-word tokens (see lecture video on sub-word tokenisation).\n",
    "\n",
    "For example, for the sentence: **This is a pretrained model.** our previous word-based tokenizer will generate the following tokens:\n",
    "\n",
    "**\"this\", \"is\", \"a\", \"pretrained\", \"model\", \".\"**\n",
    "\n",
    "Then you will find out that the word token \"pretrained\" is not in the GloVe word dictionary. Thus we can not assign a proper word vector for \"pretrained\".\n",
    "\n",
    "In BERT, the BERT tokenizer will separate the word \"pretrained\" into three sub-word tokens:\n",
    "\n",
    "**'pre', '##train', '##ed'**\n",
    "\n",
    "This way, BERT can use these three token vectors to represent the word \"pretrained\". Without the BERT tokenizer, it is hard to separate these unknown words properly.\n",
    "\n",
    "You will also see that the BERT tokenizer adds the special sentence [CLS] token and sentence separator [SEP] tokens (see this week's lecture videos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRoKe2DKyi41",
    "outputId": "cda0335f-366f-41b6-800e-777aba366922"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'capital', 'of', 'france', 'is', '[MASK]', '.'] \n",
      "\n",
      "['this', 'is', 'a', 'pre', '##train', '##ed', 'model', '.'] \n",
      "\n",
      "[ 101 1996 3007 1997 2605 2003  103 1012  102    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['[CLS]', 'the', 'capital', 'of', 'france', 'is', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] \n",
      "\n",
      "[ 101 1996 3007 1997 2605 2003  103 1012  102]\n",
      "[1 1 1 1 1 1 1 1 1]\n",
      "['[CLS]', 'the', 'capital', 'of', 'france', 'is', '[MASK]', '.', '[SEP]'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.tokenize(\"The capital of France is [MASK].\")\n",
    "print(inputs,'\\n')\n",
    "\n",
    "inputs = tokenizer.tokenize(\"This is a pretrained model.\")\n",
    "print(inputs,'\\n')\n",
    "\n",
    "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer)\n",
    "print(ids)\n",
    "print(masks)\n",
    "print(tokenizer.convert_ids_to_tokens(ids),\"\\n\")\n",
    "\n",
    "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer, pad_to_max_length=False)\n",
    "print(ids)\n",
    "print(masks)\n",
    "print(tokenizer.convert_ids_to_tokens(ids),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6OuZAA8sbdg"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvPQvgvPv1W"
   },
   "source": [
    "### Downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EundMtGPpCdf"
   },
   "source": [
    "Similar to lab 4, we need to download and preprocess the data first. The data download code is consistent with lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyuSzkafqNca",
    "outputId": "0abf49fe-cf81-4da4-9ed9-5d78ab99fc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 11186\n",
      "Test entries: 1336\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def downloadfile(url):\n",
    "  rq = requests.get(url)\n",
    "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/train.xml')\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/val.xml')\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/test.xml')\n",
    "\n",
    "\n",
    "# The code is modified from https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data_process/utils.py\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def parse_sentence_term(path, lowercase=False):\n",
    "    tree = parse(path)\n",
    "    sentences = tree.getroot()\n",
    "    data = []\n",
    "    split_char = '__split__'\n",
    "    for sentence in sentences:\n",
    "        text = sentence.find('text')\n",
    "        if text is None:\n",
    "            continue\n",
    "        text = text.text\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        aspectTerms = sentence.find('aspectTerms')\n",
    "        if aspectTerms is None:\n",
    "            continue\n",
    "        for aspectTerm in aspectTerms:\n",
    "            term = aspectTerm.get('term')\n",
    "            if lowercase:\n",
    "                term = term.lower()\n",
    "            polarity = aspectTerm.get('polarity')\n",
    "            start = aspectTerm.get('from')\n",
    "            end = aspectTerm.get('to')\n",
    "            piece = [text , term,  polarity , start , end]\n",
    "            data.append(piece)\n",
    "    return data\n",
    "train = parse_sentence_term(\"train.xml\",True)\n",
    "dev = parse_sentence_term(\"val.xml\",True)\n",
    "test = parse_sentence_term(\"test.xml\",True)\n",
    "\n",
    "print(\"Training entries: {}\".format(len(train)))\n",
    "print(\"Test entries: {}\".format(len(test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U4iCV9-rmay"
   },
   "source": [
    "We now can start playing around with the data, let’s first see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-gjWRAuqg5s",
    "outputId": "6572e32c-7e82-4c75-c176-8da121c76191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE \t ASPECT \t LABLE \t ASPECT-START-INDEX \t ASPECT-END-INDEX\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
      "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
      "['when tables opened up, the manager sat another party before us.', 'manager', 'negative', '27', '34']\n"
     ]
    }
   ],
   "source": [
    "print(\"SENTENCE \\t ASPECT \\t LABLE \\t ASPECT-START-INDEX \\t ASPECT-END-INDEX\")\n",
    "print(train[0])\n",
    "print(train[1])\n",
    "print(train[2])\n",
    "print(train[3])\n",
    "print(train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvuu4KhStqei"
   },
   "source": [
    "According to the BERT tokenize function above, we can convert the tweet text and topic words to integers:\n",
    "\n",
    "(Note: the BERT tokenize function is different from lab 4.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMCH1OoDrSNR",
    "outputId": "8c720757-5d7d-44c2-dc13-f2745f3a33eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dev_aspect_int[0]:\n",
      "[ 101 8974  102    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "x_dev_aspect_masks[0]:\n",
      "[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "x_dev_review_int[0]:\n",
      "[  101  2044  1037  3232  1997  8974  1010  1996 18726  1011  1011  1045\n",
      "  2066  1996 27940  1013 24792  2621  4897  1998  1996 13675 11514  6508\n",
      " 26852  1011  1011  2175  2091  2307  1012   102     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "x_dev_review_masks[0]:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to generate the following data\n",
    "x_train_review_int = []\n",
    "x_train_review_masks = []\n",
    "x_train_aspect_int = []\n",
    "x_train_aspect_masks = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "  # Tokenise every sentence in the training set and append the ids and masks\n",
    "  ids_train, masks_train, segments_train = tokenize(train[i][0], tokenizer)\n",
    "  x_train_review_int.append(ids_train)\n",
    "  x_train_review_masks.append(masks_train)\n",
    "  # Tokenise every aspect in the training set and append the ids and masks\n",
    "  ids_train, masks_train, segments_train = tokenize(train[i][1], tokenizer)\n",
    "  x_train_aspect_int.append(ids_train)\n",
    "  x_train_aspect_masks.append(masks_train)\n",
    "\n",
    "x_dev_review_int = []\n",
    "x_dev_review_masks = []\n",
    "x_dev_aspect_int = []\n",
    "x_dev_aspect_masks = []\n",
    "for i in range(len(dev)):\n",
    "  # Tokenise every sentence in the dev set and append the ids and masks\n",
    "  ids_dev, masks_dev, segments_dev = tokenize(dev[i][0], tokenizer)\n",
    "  x_dev_review_int.append(ids_dev)\n",
    "  x_dev_review_masks.append(masks_dev)\n",
    "  # Tokenise every aspect in the dev set and append the ids and masks\n",
    "  ids_dev, masks_dev, segments_dev = tokenize(dev[i][1], tokenizer)\n",
    "  x_dev_aspect_int.append(ids_dev)\n",
    "  x_dev_aspect_masks.append(masks_dev)\n",
    "\n",
    "x_test_review_int = []\n",
    "x_test_review_masks = []\n",
    "x_test_aspect_int = []\n",
    "x_test_aspect_masks = []\n",
    "for i in range(len(test)):\n",
    "  # Tokenise every sentence in the test set and append the ids and masks\n",
    "  ids_test, masks_test, segments_test = tokenize(test[i][0], tokenizer)\n",
    "  x_test_review_int.append(ids_test)\n",
    "  x_test_review_masks.append(masks_test)\n",
    "  # Tokenise every aspect in the test set and append the ids and masks\n",
    "  ids_test, masks_test, segments_test = tokenize(test[i][1], tokenizer)\n",
    "  x_test_aspect_int.append(ids_test)\n",
    "  x_test_aspect_masks.append(masks_test)\n",
    "\n",
    "\n",
    "#######################TODO: REMOVE FOLLOWING CODE####################\n",
    "\n",
    "\n",
    "#######################TODO: REMOVE ABOVE CODE####################\n",
    "\n",
    "\n",
    "# If use the previous tokenize function, you can get a print result like:\n",
    "assert len(x_train_aspect_int) == len(train)\n",
    "assert len(x_train_aspect_masks) == len(x_train_aspect_int)\n",
    "assert len(x_test_aspect_int) == len(test)\n",
    "assert len(x_test_aspect_masks) == len(x_test_aspect_int)\n",
    "print(\"x_dev_aspect_int[0]:\")\n",
    "print(x_dev_aspect_int[0])\n",
    "print(\"x_dev_aspect_masks[0]:\")\n",
    "print(x_dev_aspect_masks[0])\n",
    "print(\"x_dev_review_int[0]:\")\n",
    "print(x_dev_review_int[0])\n",
    "print(\"x_dev_review_masks[0]:\")\n",
    "print(x_dev_review_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IreFXgruZot"
   },
   "source": [
    "We one-hot encode the labels, using 4 (Binary:100) to represent \"positive\", 2 (Binary:010) for \"neutral\", and 1 (Binary:001) for \"negative\". Then we can convert the labels to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abIb7Fe5u3GQ",
    "outputId": "46df0588-0d46-468a-f2f7-73e066ef3508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "[1 0 0]\n",
      "[1 0 0]\n",
      "[0 1 0]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def label2int(dataset):\n",
    "  y = []\n",
    "  for example in dataset:\n",
    "    if example[2].lower() == \"negative\":\n",
    "      y.append([0,0,1])\n",
    "    elif example[2].lower() == \"neutral\":\n",
    "      y.append([0,1,0])\n",
    "    else:\n",
    "      # assert example[2].lower() == \"positive\"\n",
    "      y.append([1,0,0])\n",
    "  return y\n",
    "  \n",
    "y_train = label2int(train)\n",
    "y_dev = label2int(dev)\n",
    "y_test = label2int(test)\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])\n",
    "print(y_train[3])\n",
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnnSuspvC5b"
   },
   "source": [
    "Now we have almost done the data preprocessing. Unlike the previous lab1-lab4, there are two x (review and aspect) to input the model. The easiest way is to combine the review and aspect into one sentence and then input it into the model. Thus we can use the previous model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKOiVVXQu-_I",
    "outputId": "368c14c7-3217-406a-9d56-87ecda8981a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2044  1037  3232  1997  8974  1010  1996 18726  1011  1011  1045\n",
      "  2066  1996 27940  1013 24792  2621  4897  1998  1996 13675 11514  6508\n",
      " 26852  1011  1011  2175  2091  2307  1012  1026 19802  1028  8974   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
      "\n",
      "[  101  2044  1037  3232  1997  8974  1010  1996 18726  1011  1011  1045\n",
      "  2066  1996 27940  1013 24792  2621  4897  1998  1996 13675 11514  6508\n",
      " 26852  1011  1011  2175  2091  2307  1012  1026 19802  1028  8974   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to combine the tweet and topic into the following varibles\n",
    "x_train_int = []\n",
    "x_train_masks = []\n",
    "for row in train:\n",
    "  # Combining the review and aspect of training data with <SEP> as special token and then tokenizing them\n",
    "  ids_train, masks_train, segments_train = tokenize(row[0] + \"<SEP>\" + row[1], tokenizer)\n",
    "  x_train_int.append(ids_train)\n",
    "  x_train_masks.append(masks_train)\n",
    "\n",
    "x_dev_int = []\n",
    "x_dev_masks = []\n",
    "for row in dev:\n",
    "  # Combining the review and aspect with <SEP> as special token and then tokenizing them\n",
    "  ids_dev, masks_dev, segments_dev = tokenize(row[0] + \"<SEP>\" + row[1], tokenizer)\n",
    "  x_dev_int.append(ids_dev)\n",
    "  x_dev_masks.append(masks_dev)\n",
    "\n",
    "x_test_int = []\n",
    "x_test_masks = []\n",
    "for row in test:\n",
    "  # Combining the review and aspect with <SEP> as special token and then tokenizing them\n",
    "  ids_test, masks_test, segments_test = tokenize(row[0] + \"<SEP>\" + row[1], tokenizer)\n",
    "  x_test_int.append(ids_test)\n",
    "  x_test_masks.append(masks_test)\n",
    "\n",
    "# Tips: \n",
    "# 1) We can use the special token <SEP> to concatenate the tweets and topics.\n",
    "# 2) After combine them, make sure they are paded.\n",
    "\n",
    "#######################TODO: REMOVE FOLLOWING CODE####################\n",
    "\n",
    "\n",
    "#######################TODO: REMOVE ABOVE CODE####################\n",
    "\n",
    "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function\n",
    "x_train_int_np = np.array(x_train_int)\n",
    "x_train_masks_np = np.array(x_train_masks)\n",
    "x_dev_int_np = np.array(x_dev_int)\n",
    "x_dev_masks_np = np.array(x_dev_masks)\n",
    "x_test_int_np = np.array(x_test_int)\n",
    "x_test_masks_np = np.array(x_test_masks)\n",
    "\n",
    "\n",
    "print(x_dev_int[0])\n",
    "print(x_dev_masks[0],'\\n')\n",
    "print(x_dev_int_np[0])\n",
    "print(x_dev_masks_np[0]) # senetnce + aspect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqvUGIwwGJqu"
   },
   "source": [
    "## Model 1: Prebuilt Sequence Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSxC41ln07im"
   },
   "source": [
    "The huggingface transformer package provides many prebuilt models. Now let us try a sequence classification model based on distillBERT. \n",
    "\n",
    "The models with BERT are much bigger than our previous models. To run it faster, we can use TPU here. The detailed guideline about using TPU can be found from https://www.tensorflow.org/guide/tpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867,
     "referenced_widgets": [
      "684a4f37e5604db4acdef92a32dea21d",
      "81d1f4f5569a4a569a21d990690f1d97",
      "e25137fc62154309913ef57b76af5b18",
      "f8b8cc53c5d6402bac25801bc14643ad",
      "ab0530457e6743958b9efddb2b8b6ff1",
      "310f91c3537e493e9a1ccbc01492af40",
      "ea760351988e4efcab927ea81f61427c",
      "ee52e64a9b3d4beea4650f8cdb068e72",
      "f014ed3e698f48e7bab5580b600d47d3",
      "7d745df6af564ce79b8ef5e929af72d7",
      "c5fb55773f0942f3a75e0e13d4934861"
     ]
    },
    "id": "1gXFbb2cxBlw",
    "outputId": "37cc8ebb-10ce-4e97-93de-844fbc0c24b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.195.218:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.195.218:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684a4f37e5604db4acdef92a32dea21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "\n",
    "config = DistilBertConfig(num_labels=3)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "def create_TFDistilBertForSequenceClassification():\n",
    "  transformer_model = TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config)\n",
    "  input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "  input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n",
    "  X = transformer_model(input_ids, input_masks_ids)\n",
    "  return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n",
    "\n",
    "use_tpu = True\n",
    "if use_tpu:\n",
    "  # Create distribution strategy\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "  # Create model on TPU:\n",
    "  with strategy.scope():\n",
    "    model = create_TFDistilBertForSequenceClassification()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "  model = create_TFDistilBertForSequenceClassification()\n",
    "  model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CPFj0CMx9mw",
    "outputId": "87234bef-96d8-4b90-d7f1-a0c4d634029e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_for_sequence_cl  TFSequenceClassifie  66955779   ['input_token[0][0]',            \n",
      " assification (TFDistilBertForS  rOutput(loss=None,               'masked_token[0][0]']           \n",
      " equenceClassification)         logits=(None, 3),                                                 \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQQH5lE_33Vn",
    "outputId": "4cc8137b-845e-4b6e-9e21-eda0944ea3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 106s 3s/step - loss: 1.1483 - accuracy: 0.3848 - val_loss: 0.6060 - val_accuracy: 0.4505\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.6040 - accuracy: 0.4458 - val_loss: 0.5740 - val_accuracy: 0.4700\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.5732 - accuracy: 0.4904 - val_loss: 0.5510 - val_accuracy: 0.5375\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.5257 - accuracy: 0.5913 - val_loss: 0.4809 - val_accuracy: 0.6637\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.4934 - accuracy: 0.6628 - val_loss: 0.4764 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.5615 - accuracy: 0.5148 - val_loss: 0.4697 - val_accuracy: 0.6599\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.4465 - accuracy: 0.7340 - val_loss: 0.3887 - val_accuracy: 0.7643\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.3622 - accuracy: 0.7908 - val_loss: 0.4436 - val_accuracy: 0.7763\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.3682 - accuracy: 0.8033 - val_loss: 0.4659 - val_accuracy: 0.7207\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.3664 - accuracy: 0.7901 - val_loss: 0.4805 - val_accuracy: 0.7770\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 0.3515 - accuracy: 0.8227 - val_loss: 0.4540 - val_accuracy: 0.7575\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.3242 - accuracy: 0.8251 - val_loss: 0.5383 - val_accuracy: 0.7710\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2828 - accuracy: 0.8537 - val_loss: 0.5742 - val_accuracy: 0.7252\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.4094 - accuracy: 0.7298 - val_loss: 0.4863 - val_accuracy: 0.7725\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.3208 - accuracy: 0.8561 - val_loss: 0.5130 - val_accuracy: 0.7673\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2646 - accuracy: 0.8719 - val_loss: 0.4893 - val_accuracy: 0.7838\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.2245 - accuracy: 0.8805 - val_loss: 0.5547 - val_accuracy: 0.7680\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2813 - accuracy: 0.8476 - val_loss: 0.5424 - val_accuracy: 0.7943\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.2243 - accuracy: 0.8993 - val_loss: 0.7889 - val_accuracy: 0.7965\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.1854 - accuracy: 0.9081 - val_loss: 0.7251 - val_accuracy: 0.8026\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2501 - accuracy: 0.8871 - val_loss: 0.4827 - val_accuracy: 0.7778\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2433 - accuracy: 0.8889 - val_loss: 0.5957 - val_accuracy: 0.8011\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.1947 - accuracy: 0.9118 - val_loss: 0.6868 - val_accuracy: 0.7965\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.1547 - accuracy: 0.9304 - val_loss: 0.8660 - val_accuracy: 0.8026\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.1299 - accuracy: 0.9455 - val_loss: 0.9412 - val_accuracy: 0.7988\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.1399 - accuracy: 0.9405 - val_loss: 0.8822 - val_accuracy: 0.7988\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.1108 - accuracy: 0.9534 - val_loss: 0.9944 - val_accuracy: 0.8003\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.1377 - accuracy: 0.9363 - val_loss: 0.6940 - val_accuracy: 0.7973\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2632 - accuracy: 0.8768 - val_loss: 0.6044 - val_accuracy: 0.7718\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.1872 - accuracy: 0.9240 - val_loss: 0.8756 - val_accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train_int_np,x_train_masks_np],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQcytuBdaWVp",
    "outputId": "71b08475-8341-48b5-ae78-bf48e7a36c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 8s 98ms/step - loss: 0.8981 - accuracy: 0.7987\n",
      "[0.8980957865715027, 0.798652708530426]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdZ4nl08vp9A"
   },
   "source": [
    "\n",
    "## Model 2: Neural bag of words using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gyCwXFj_R5w"
   },
   "source": [
    "We use model3-1 from lab4 to integrate BERT, using BERT instead of the previous static word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DStlnRQRf-4v"
   },
   "outputs": [],
   "source": [
    "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
    "    def call(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "        else:\n",
    "            return super().call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fTwmYDvNEyT"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "def get_BERT_layer():\n",
    "  distil_bert = 'distilbert-base-uncased'\n",
    "  config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "  config.output_hidden_states = False\n",
    "  return TFDistilBertModel.from_pretrained(distil_bert, config = config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VICS9rY8C7KH",
    "outputId": "07d50633-ade0-4783-b820-2fcef171fd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.55.195.218:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.55.195.218:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.195.218:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.195.218:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model2_BERT\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_token[0][0]',            \n",
      " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
      "                                one, 128, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_maske  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
      " d (GlobalAveragePooling1DMaske                                                                   \n",
      " d)                                                                                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           12304       ['global_average_pooling1d_masked\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            51          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,375,235\n",
      "Trainable params: 66,375,235\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hdepth=16\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "EMBED_SIZE=100\n",
    "\n",
    "\n",
    "def create_bag_of_words_BERT():\n",
    "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "  bert_embeddings = get_BERT_layer()\n",
    "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "\n",
    "  pooled_sent=GlobalAveragePooling1DMasked()(embedded_sent)\n",
    "  hidden_output=Dense(hdepth,input_shape=(MAX_SEQUENCE_LENGTH,EMBED_SIZE),activation='sigmoid',kernel_initializer='glorot_uniform')(pooled_sent) # Sigmoid\n",
    "  label=Dense(3,input_shape=(hdepth,),activation='softmax',kernel_initializer='glorot_uniform')(hidden_output)\n",
    "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
    "\n",
    "use_tpu = True\n",
    "if use_tpu:\n",
    "  # Create distribution strategy\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "  # Create model\n",
    "  with strategy.scope():\n",
    "    model2 = create_bag_of_words_BERT()\n",
    "    optimizer2 = tf.keras.optimizers.Adam(lr=5e-5)\n",
    "    model2.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "  model2 = create_bag_of_words_BERT()\n",
    "  model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.summary() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0SbsCsxF1zi",
    "outputId": "b26441b7-6d49-4024-9638-e3de84ae4a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 106s 2s/step - loss: 0.6057 - accuracy: 0.4554 - val_loss: 0.5514 - val_accuracy: 0.5901\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.5206 - accuracy: 0.6231 - val_loss: 0.4799 - val_accuracy: 0.6884\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 0.4529 - accuracy: 0.7353 - val_loss: 0.4320 - val_accuracy: 0.7553\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.4089 - accuracy: 0.7980 - val_loss: 0.4103 - val_accuracy: 0.8041\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.3844 - accuracy: 0.8381 - val_loss: 0.4027 - val_accuracy: 0.8056\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.3646 - accuracy: 0.8670 - val_loss: 0.3975 - val_accuracy: 0.8153\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.3502 - accuracy: 0.8852 - val_loss: 0.3942 - val_accuracy: 0.8198\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.3376 - accuracy: 0.9025 - val_loss: 0.3925 - val_accuracy: 0.8236\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 0.3280 - accuracy: 0.9154 - val_loss: 0.3943 - val_accuracy: 0.8131\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.3196 - accuracy: 0.9260 - val_loss: 0.3924 - val_accuracy: 0.8161\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.3127 - accuracy: 0.9333 - val_loss: 0.3892 - val_accuracy: 0.8228\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.3066 - accuracy: 0.9392 - val_loss: 0.3943 - val_accuracy: 0.8206\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.3013 - accuracy: 0.9442 - val_loss: 0.3879 - val_accuracy: 0.8273\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2962 - accuracy: 0.9504 - val_loss: 0.3896 - val_accuracy: 0.8251\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.2914 - accuracy: 0.9565 - val_loss: 0.3933 - val_accuracy: 0.8221\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.2875 - accuracy: 0.9598 - val_loss: 0.3863 - val_accuracy: 0.8251\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2866 - accuracy: 0.9581 - val_loss: 0.3900 - val_accuracy: 0.8236\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2832 - accuracy: 0.9597 - val_loss: 0.3877 - val_accuracy: 0.8251\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2778 - accuracy: 0.9665 - val_loss: 0.3879 - val_accuracy: 0.8168\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.2758 - accuracy: 0.9663 - val_loss: 0.3879 - val_accuracy: 0.8221\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.2725 - accuracy: 0.9699 - val_loss: 0.3891 - val_accuracy: 0.8183\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2696 - accuracy: 0.9720 - val_loss: 0.3816 - val_accuracy: 0.8288\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.2692 - accuracy: 0.9701 - val_loss: 0.3837 - val_accuracy: 0.8228\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.2663 - accuracy: 0.9734 - val_loss: 0.3843 - val_accuracy: 0.8221\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.2638 - accuracy: 0.9749 - val_loss: 0.3857 - val_accuracy: 0.8213\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2615 - accuracy: 0.9761 - val_loss: 0.3812 - val_accuracy: 0.8258\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2610 - accuracy: 0.9751 - val_loss: 0.3782 - val_accuracy: 0.8296\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.2591 - accuracy: 0.9760 - val_loss: 0.3813 - val_accuracy: 0.8221\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 6s 256ms/step - loss: 0.2564 - accuracy: 0.9775 - val_loss: 0.3774 - val_accuracy: 0.8303\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.2557 - accuracy: 0.9774 - val_loss: 0.3824 - val_accuracy: 0.8183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model2.fit([x_train_int_np,x_train_masks_np],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rs0_vvG6UQtv",
    "outputId": "459267b8-d7d2-4155-ce60-73b3f81101f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 8s 104ms/step - loss: 0.3774 - accuracy: 0.8301\n",
      "[0.3773697018623352, 0.8300898671150208]\n"
     ]
    }
   ],
   "source": [
    "results = model2.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awOphcCnhEwv"
   },
   "source": [
    "## Model 3: CNN or LSTM with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5codSzohQ_9"
   },
   "source": [
    "Please follow the same methods as model2 to construct a CNN or LSTM model on top of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMiiWhW4hPRA",
    "outputId": "85335247-b049-4aa5-c1f7-da681603153c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.55.195.218:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.55.195.218:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.195.218:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.195.218:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model2_BERT\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_6 (TFDist  TFBaseModelOutput(l  66362880   ['input_token[0][0]',            \n",
      " ilBertModel)                   ast_hidden_state=(N               'masked_token[0][0]']           \n",
      "                                one, 128, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 100)          347600      ['tf_distil_bert_model_6[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 3)            303         ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,710,783\n",
      "Trainable params: 66,710,783\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "hdepth=16\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "EMBED_SIZE=100\n",
    "\n",
    "def create_bag_of_words_BERT_CNN():\n",
    "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "  bert_embeddings = get_BERT_layer()\n",
    "  # Embedding Layer\n",
    "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "  # LSTM Layer\n",
    "  lstm_layer = LSTM(units=100)(embedded_sent)\n",
    "  # Output Layer\n",
    "  label=Dense(3,input_shape=(hdepth,),activation='softmax',kernel_initializer='glorot_uniform')(lstm_layer)\n",
    "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
    "\n",
    "use_tpu = True\n",
    "if use_tpu:\n",
    "  # Create distribution strategy\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "  # Create model\n",
    "  with strategy.scope():\n",
    "    model3 = create_bag_of_words_BERT_CNN()\n",
    "    optimizer3 = tf.keras.optimizers.Adam(lr=5e-5)\n",
    "    model3.compile(optimizer=optimizer3, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "  model3 = create_bag_of_words_BERT_CNN()\n",
    "  model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model3.summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9HmJNzNpIEM",
    "outputId": "b877acdd-0b83-4cdf-8421-de292ab6caa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 114s 3s/step - loss: 0.5893 - accuracy: 0.4840 - val_loss: 0.5212 - val_accuracy: 0.5901\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 0.4502 - accuracy: 0.6725 - val_loss: 0.3595 - val_accuracy: 0.7665\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 6s 290ms/step - loss: 0.3283 - accuracy: 0.7871 - val_loss: 0.3242 - val_accuracy: 0.7868\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 6s 290ms/step - loss: 0.2694 - accuracy: 0.8349 - val_loss: 0.2957 - val_accuracy: 0.8198\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.2239 - accuracy: 0.8696 - val_loss: 0.3116 - val_accuracy: 0.8108\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.1943 - accuracy: 0.8887 - val_loss: 0.3097 - val_accuracy: 0.8071\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 0.1584 - accuracy: 0.9118 - val_loss: 0.3282 - val_accuracy: 0.8191\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.1340 - accuracy: 0.9282 - val_loss: 0.3381 - val_accuracy: 0.8198\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 6s 290ms/step - loss: 0.1082 - accuracy: 0.9429 - val_loss: 0.3758 - val_accuracy: 0.8131\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 0.0993 - accuracy: 0.9489 - val_loss: 0.3720 - val_accuracy: 0.8146\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.0787 - accuracy: 0.9616 - val_loss: 0.3882 - val_accuracy: 0.8108\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 6s 294ms/step - loss: 0.0705 - accuracy: 0.9668 - val_loss: 0.4278 - val_accuracy: 0.8071\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0635 - accuracy: 0.9692 - val_loss: 0.4311 - val_accuracy: 0.8108\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 0.0562 - accuracy: 0.9743 - val_loss: 0.4110 - val_accuracy: 0.8198\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.0490 - accuracy: 0.9798 - val_loss: 0.4117 - val_accuracy: 0.8251\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 6s 294ms/step - loss: 0.0458 - accuracy: 0.9793 - val_loss: 0.4373 - val_accuracy: 0.8116\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.0392 - accuracy: 0.9834 - val_loss: 0.4464 - val_accuracy: 0.8138\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0362 - accuracy: 0.9841 - val_loss: 0.4600 - val_accuracy: 0.8138\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0356 - accuracy: 0.9849 - val_loss: 0.4769 - val_accuracy: 0.8093\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0324 - accuracy: 0.9861 - val_loss: 0.4656 - val_accuracy: 0.8228\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.4733 - val_accuracy: 0.8213\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 6s 295ms/step - loss: 0.0313 - accuracy: 0.9863 - val_loss: 0.4494 - val_accuracy: 0.8228\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 6s 290ms/step - loss: 0.0254 - accuracy: 0.9895 - val_loss: 0.4924 - val_accuracy: 0.8131\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 6s 290ms/step - loss: 0.0219 - accuracy: 0.9920 - val_loss: 0.4814 - val_accuracy: 0.8213\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0235 - accuracy: 0.9909 - val_loss: 0.4869 - val_accuracy: 0.8273\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0252 - accuracy: 0.9894 - val_loss: 0.4719 - val_accuracy: 0.8161\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 0.0229 - accuracy: 0.9907 - val_loss: 0.4611 - val_accuracy: 0.8243\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 6s 295ms/step - loss: 0.0213 - accuracy: 0.9911 - val_loss: 0.4811 - val_accuracy: 0.8243\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.0177 - accuracy: 0.9938 - val_loss: 0.5185 - val_accuracy: 0.8176\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.0195 - accuracy: 0.9920 - val_loss: 0.5000 - val_accuracy: 0.8176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model3.fit([x_train_int_np,x_train_masks_np],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7TXLjQhpY--",
    "outputId": "182f179a-0b13-4c73-89bf-66f2ef59c366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 10s 114ms/step - loss: 0.4874 - accuracy: 0.8383\n",
      "[0.4874451458454132, 0.8383233547210693]\n"
     ]
    }
   ],
   "source": [
    "results = model3.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "7001_2021_22_lab8_Aspect-Based Sentiment Analysis with BERT without answer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a5376e1c80b440fa241b974e2cf656f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3622e198da148a9b1bfcc14f5b27225",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_831498ac7a1d4b59a1c1174df5d2a7b0",
      "value": 483
     }
    },
    "0b454e3aa10449bab0e12a0596956a76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9360dea7d34a4990936148f6169c3824",
       "IPY_MODEL_922efe9c76364b7b8f34cd096954bbf4",
       "IPY_MODEL_0ee4a6d9b79447a6b9261b36136715d7"
      ],
      "layout": "IPY_MODEL_ea6d50c398b84282a48de27658bcbb9d"
     }
    },
    "0ee4a6d9b79447a6b9261b36136715d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c747fa346cb7406784f7aacf5bb511f4",
      "placeholder": "​",
      "style": "IPY_MODEL_2567d8842c18498695baab8eb5a3cfc4",
      "value": " 226k/226k [00:00&lt;00:00, 757kB/s]"
     }
    },
    "2567d8842c18498695baab8eb5a3cfc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f84d0392d2240ee8ab110b14f147135": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310f91c3537e493e9a1ccbc01492af40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c04dde3e07049779e31da6db6c621da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcce4986a120492b830dadcfe16d8b61",
      "placeholder": "​",
      "style": "IPY_MODEL_66d5c015f9e34865aa14526f4f2b2381",
      "value": "Downloading: 100%"
     }
    },
    "3fec1d86d6e741c6942ca6fe379681ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76af8cde39884aaa8b90716f60249141",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_484a90ceaf5840cc95dc8fee9852a9f7",
      "value": 28
     }
    },
    "425468b3eed24365aad81b0e40119d16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "484a90ceaf5840cc95dc8fee9852a9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "63a31d3ca8ea4d82acf7bbc99f9eaec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e94f80f6c6864a40ba4f423277582fdd",
      "placeholder": "​",
      "style": "IPY_MODEL_425468b3eed24365aad81b0e40119d16",
      "value": " 28.0/28.0 [00:00&lt;00:00, 177B/s]"
     }
    },
    "64fedb8aa29f4d598c2402b421fb95ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66d5c015f9e34865aa14526f4f2b2381": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "684a4f37e5604db4acdef92a32dea21d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81d1f4f5569a4a569a21d990690f1d97",
       "IPY_MODEL_e25137fc62154309913ef57b76af5b18",
       "IPY_MODEL_f8b8cc53c5d6402bac25801bc14643ad"
      ],
      "layout": "IPY_MODEL_ab0530457e6743958b9efddb2b8b6ff1"
     }
    },
    "68cfc6b4ef8f4e8383453cd37c0f4261": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76af8cde39884aaa8b90716f60249141": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bb3662f2166440f91e513d50d95fbe6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d745df6af564ce79b8ef5e929af72d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81d1f4f5569a4a569a21d990690f1d97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_310f91c3537e493e9a1ccbc01492af40",
      "placeholder": "​",
      "style": "IPY_MODEL_ea760351988e4efcab927ea81f61427c",
      "value": "Downloading: 100%"
     }
    },
    "831498ac7a1d4b59a1c1174df5d2a7b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "922efe9c76364b7b8f34cd096954bbf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f188e328fb3c45868a0b581fdf477a48",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc3387a8d78d479e973f45e958a79cef",
      "value": 231508
     }
    },
    "9360dea7d34a4990936148f6169c3824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bb3662f2166440f91e513d50d95fbe6",
      "placeholder": "​",
      "style": "IPY_MODEL_b362747df7ce429590a80af9acac1a0f",
      "value": "Downloading: 100%"
     }
    },
    "9e148d92e6e740bcb80e95890cb8ba4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0bf941ef39d4e648cb2e8f50cd6c5e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab0530457e6743958b9efddb2b8b6ff1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b362747df7ce429590a80af9acac1a0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3438d18a2bb4e29979a708a1b763f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cdbd7d8b506c4209a6ffccc25e927522",
       "IPY_MODEL_0a5376e1c80b440fa241b974e2cf656f",
       "IPY_MODEL_f5f557c5831b4e839d30d7ee3ba129ed"
      ],
      "layout": "IPY_MODEL_fe244c1ce9d24c1680a1f1ff382b2c32"
     }
    },
    "c5fb55773f0942f3a75e0e13d4934861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c747fa346cb7406784f7aacf5bb511f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdbd7d8b506c4209a6ffccc25e927522": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f84d0392d2240ee8ab110b14f147135",
      "placeholder": "​",
      "style": "IPY_MODEL_64fedb8aa29f4d598c2402b421fb95ad",
      "value": "Downloading: 100%"
     }
    },
    "cf7d0ccad32c46a18eeb56f53bd7ee68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c04dde3e07049779e31da6db6c621da",
       "IPY_MODEL_3fec1d86d6e741c6942ca6fe379681ff",
       "IPY_MODEL_63a31d3ca8ea4d82acf7bbc99f9eaec8"
      ],
      "layout": "IPY_MODEL_9e148d92e6e740bcb80e95890cb8ba4b"
     }
    },
    "d3622e198da148a9b1bfcc14f5b27225": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e25137fc62154309913ef57b76af5b18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee52e64a9b3d4beea4650f8cdb068e72",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f014ed3e698f48e7bab5580b600d47d3",
      "value": 363423424
     }
    },
    "e94f80f6c6864a40ba4f423277582fdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea6d50c398b84282a48de27658bcbb9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea760351988e4efcab927ea81f61427c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee52e64a9b3d4beea4650f8cdb068e72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f014ed3e698f48e7bab5580b600d47d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f188e328fb3c45868a0b581fdf477a48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5f557c5831b4e839d30d7ee3ba129ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68cfc6b4ef8f4e8383453cd37c0f4261",
      "placeholder": "​",
      "style": "IPY_MODEL_a0bf941ef39d4e648cb2e8f50cd6c5e8",
      "value": " 483/483 [00:00&lt;00:00, 3.44kB/s]"
     }
    },
    "f8b8cc53c5d6402bac25801bc14643ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d745df6af564ce79b8ef5e929af72d7",
      "placeholder": "​",
      "style": "IPY_MODEL_c5fb55773f0942f3a75e0e13d4934861",
      "value": " 347M/347M [00:09&lt;00:00, 39.9MB/s]"
     }
    },
    "fc3387a8d78d479e973f45e958a79cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fcce4986a120492b830dadcfe16d8b61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe244c1ce9d24c1680a1f1ff382b2c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
